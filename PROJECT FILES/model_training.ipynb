{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce901830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# CleanTech Waste Classification Model Training\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates the complete process of training a waste classification model using VGG16 transfer learning.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Import Required Libraries\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from tensorflow.keras.applications import VGG16\\n\",\n",
    "    \"from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\\n\",\n",
    "    \"from tensorflow.keras.models import Model\\n\",\n",
    "    \"from tensorflow.keras.optimizers import Adam\\n\",\n",
    "    \"from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\\n\",\n",
    "    \"from tensorflow.keras.utils import to_categorical\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"Keras version: {tf.keras.__version__}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Define Configuration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Configuration\\n\",\n",
    "    \"WASTE_CATEGORIES = ['organic', 'recyclable', 'hazardous', 'general']\\n\",\n",
    "    \"IMG_SIZE = (224, 224)\\n\",\n",
    "    \"BATCH_SIZE = 32\\n\",\n",
    "    \"EPOCHS = 10\\n\",\n",
    "    \"LEARNING_RATE = 0.0001\\n\",\n",
    "    \"VALIDATION_SPLIT = 0.2\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Waste Categories: {WASTE_CATEGORIES}\\\")\\n\",\n",
    "    \"print(f\\\"Number of Classes: {len(WASTE_CATEGORIES)}\\\")\\n\",\n",
    "    \"print(f\\\"Image Size: {IMG_SIZE}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Create Synthetic Dataset\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def create_synthetic_dataset():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create synthetic waste dataset for demonstration.\\n\",\n",
    "    \"    In a real project, you would use actual waste images.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    print(\\\"Creating synthetic waste dataset...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create directories\\n\",\n",
    "    \"    data_dir = 'waste_dataset'\\n\",\n",
    "    \"    for category in WASTE_CATEGORIES:\\n\",\n",
    "    \"        os.makedirs(f'{data_dir}/{category}', exist_ok=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate synthetic images with category-specific patterns\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    images_per_category = 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, category in enumerate(WASTE_CATEGORIES):\\n\",\n",
    "    \"        print(f\\\"Generating {images_per_category} images for {category}...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for j in range(images_per_category):\\n\",\n",
    "    \"            # Create synthetic image with category-specific patterns\\n\",\n",
    "    \"            if category == 'organic':\\n\",\n",
    "    \"                img = np.random.normal(0.3, 0.1, (224, 224, 3))  # Brown/green tones\\n\",\n",
    "    \"                img[:, :, 1] *= 1.5  # More green\\n\",\n",
    "    \"            elif category == 'recyclable':\\n\",\n",
    "    \"                img = np.random.normal(0.7, 0.1, (224, 224, 3))  # Blue tones\\n\",\n",
    "    \"                img[:, :, 2] *= 1.5  # More blue\\n\",\n",
    "    \"            elif category == 'hazardous':\\n\",\n",
    "    \"                img = np.random.normal(0.5, 0.1, (224, 224, 3))  # Red tones\\n\",\n",
    "    \"                img[:, :, 0] *= 1.5  # More red\\n\",\n",
    "    \"            else:  # general\\n\",\n",
    "    \"                img = np.random.normal(0.5, 0.1, (224, 224, 3))  # Gray tones\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            img = np.clip(img, 0, 1) * 255\\n\",\n",
    "    \"            img = img.astype(np.uint8)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Save image\\n\",\n",
    "    \"            image = Image.fromarray(img)\\n\",\n",
    "    \"            image.save(f'{data_dir}/{category}/waste_{j}.jpg')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Synthetic dataset created successfully!\\\")\\n\",\n",
    "    \"    return data_dir\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create the dataset\\n\",\n",
    "    \"dataset_path = create_synthetic_dataset()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Data Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize sample images from each category\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 4, figsize=(15, 8))\\n\",\n",
    "    \"fig.suptitle('Sample Images from Each Waste Category', fontsize=16)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, category in enumerate(WASTE_CATEGORIES):\\n\",\n",
    "    \"    # Load and display first image\\n\",\n",
    "    \"    img_path = f'{dataset_path}/{category}/waste_0.jpg'\\n\",\n",
    "    \"    img = load_img(img_path)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[0, i].imshow(img)\\n\",\n",
    "    \"    axes[0, i].set_title(f'{category.capitalize()} - Sample 1')\\n\",\n",
    "    \"    axes[0, i].axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load and display second image\\n\",\n",
    "    \"    img_path = f'{dataset_path}/{category}/waste_1.jpg'\\n\",\n",
    "    \"    img = load_img(img_path)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[1, i].imshow(img)\\n\",\n",
    "    \"    axes[1, i].set_title(f'{category.capitalize()} - Sample 2')\\n\",\n",
    "    \"    axes[1, i].axis('off')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display dataset statistics\\n\",\n",
    "    \"print(\\\"\\\\nDataset Statistics:\\\")\\n\",\n",
    "    \"for category in WASTE_CATEGORIES:\\n\",\n",
    "    \"    count = len(os.listdir(f'{dataset_path}/{category}'))\\n\",\n",
    "    \"    print(f\\\"{category.capitalize()}: {count} images\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Data Preprocessing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data generators with augmentation\\n\",\n",
    "    \"train_datagen = ImageDataGenerator(\\n\",\n",
    "    \"    rescale=1./255,\\n\",\n",
    "    \"    rotation_range=20,\\n\",\n",
    "    \"    width_shift_range=0.2,\\n\",\n",
    "    \"    height_shift_range=0.2,\\n\",\n",
    "    \"    horizontal_flip=True,\\n\",\n",
    "    \"    zoom_range=0.2,\\n\",\n",
    "    \"    validation_split=VALIDATION_SPLIT\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"val_datagen = ImageDataGenerator(\\n\",\n",
    "    \"    rescale=1./255,\\n\",\n",
    "    \"    validation_split=VALIDATION_SPLIT\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create data generators\\n\",\n",
    "    \"train_generator = train_datagen.flow_from_directory(\\n\",\n",
    "    \"    dataset_path,\\n\",\n",
    "    \"    target_size=IMG_SIZE,\\n\",\n",
    "    \"    batch_size=BATCH_SIZE,\\n\",\n",
    "    \"    class_mode='categorical',\\n\",\n",
    "    \"    subset='training',\\n\",\n",
    "    \"    shuffle=True\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"validation_generator = val_datagen.flow_from_directory(\\n\",\n",
    "    \"    dataset_path,\\n\",\n",
    "    \"    target_size=IMG_SIZE,\\n\",\n",
    "    \"    batch_size=BATCH_SIZE,\\n\",\n",
    "    \"    class_mode='categorical',\\n\",\n",
    "    \"    subset='validation',\\n\",\n",
    "    \"    shuffle=False\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training samples: {train_generator.samples}\\\")\\n\",\n",
    "    \"print(f\\\"Validation samples: {validation_generator.samples}\\\")\\n\",\n",
    "    \"print(f\\\"Class indices: {train_generator.class_indices}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Model Architecture\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def create_model():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create VGG16 transfer learning model for waste classification\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Load pre-trained VGG16 model\\n\",\n",
    "    \"    base_model = VGG16(\\n\",\n",
    "    \"        weights='imagenet',\\n\",\n",
    "    \"        include_top=False,\\n\",\n",
    "    \"        input_shape=(*IMG_SIZE, 3)\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Freeze base model layers\\n\",\n",
    "    \"    base_model.trainable = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add custom classification layers\\n\",\n",
    "    \"    x = base_model.output\\n\",\n",
    "    \"    x = GlobalAveragePooling2D()(x)\\n\",\n",
    "    \"    x = Dense(512, activation='relu')(x)\\n\",\n",
    "    \"    x = Dropout(0.5)(x)\\n\",\n",
    "    \"    x = Dense(256, activation='relu')(x)\\n\",\n",
    "    \"    x = Dropout(0.3)(x)\\n\",\n",
    "    \"    predictions = Dense(len(WASTE_CATEGORIES), activation='softmax')(x)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create the model\\n\",\n",
    "    \"    model = Model(inputs=base_model.input, outputs=predictions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Compile model\\n\",\n",
    "    \"    model.compile(\\n\",\n",
    "    \"        optimizer=Adam(learning_rate=LEARNING_RATE),\\n\",\n",
    "    \"        loss='categorical_crossentropy',\\n\",\n",
    "    \"        metrics=['accuracy']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create and display model\\n\",\n",
    "    \"model = create_model()\\n\",\n",
    "    \"model.summary()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Model Training\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define callbacks\\n\",\n",
    "    \"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\\n\",\n",
    "    \"\\n\",\n",
    "    \"callbacks = [\\n\",\n",
    "    \"    ModelCheckpoint(\\n\",\n",
    "    \"        'best_waste_model.h5',\\n\",\n",
    "    \"        monitor='val_accuracy',\\n\",\n",
    "    \"        save_best_only=True,\\n\",\n",
    "    \"        verbose=1\\n\",\n",
    "    \"    ),\\n\",\n",
    "    \"    EarlyStopping(\\n\",\n",
    "    \"        monitor='val_loss',\\n\",\n",
    "    \"        patience=5,\\n\",\n",
    "    \"        restore_best_weights=True\\n\",\n",
    "    \"    ),\\n\",\n",
    "    \"    ReduceLROnPlateau(\\n\",\n",
    "    \"        monitor='val_loss',\\n\",\n",
    "    \"        factor=0.5,\\n\",\n",
    "    \"        patience=3,\\n\",\n",
    "    \"        min_lr=1e-7\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train the model\\n\",\n",
    "    \"print(\\\"Starting model training...\\\")\\n\",\n",
    "    \"history = model.fit(\\n\",\n",
    "    \"    train_generator,\\n\",\n",
    "    \"    epochs=EPOCHS,\\n\",\n",
    "    \"    validation_data=validation_generator,\\n\",\n",
    "    \"    callbacks=callbacks,\\n\",\n",
    "    \"    verbose=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Training completed!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Training Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training history\\n\",\n",
    "    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot accuracy\\n\",\n",
    "    \"ax1.plot(history.history['accuracy'], label='Training Accuracy')\\n\",\n",
    "    \"ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\\n\",\n",
    "    \"ax1.set_title('Model Accuracy')\\n\",\n",
    "    \"ax1.set_xlabel('Epoch')\\n\",\n",
    "    \"ax1.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax1.legend()\\n\",\n",
    "    \"ax1.grid(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot loss\\n\",\n",
    "    \"ax2.plot(history.history['loss'], label='Training Loss')\\n\",\n",
    "    \"ax2.plot(history.history['val_loss'], label='Validation Loss')\\n\",\n",
    "    \"ax2.set_title('Model Loss')\\n\",\n",
    "    \"ax2.set_xlabel('Epoch')\\n\",\n",
    "    \"ax2.set_ylabel('Loss')\\n\",\n",
    "    \"ax2.legend()\\n\",\n",
    "    \"ax2.grid(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print final metrics\\n\",\n",
    "    \"final_train_acc = history.history['accuracy'][-1]\\n\",\n",
    "    \"final_val_acc = history.history['val_accuracy'][-1]\\n\",\n",
    "    \"final_train_loss = history.history['loss'][-1]\\n\",\n",
    "    \"final_val_loss = history.history['val_loss'][-1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nFinal Training Accuracy: {final_train_acc:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Final Validation Accuracy: {final_val_acc:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Final Training Loss: {final_train_loss:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Final Validation Loss: {final_val_loss:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Model Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate model on validation set\\n\",\n",
    "    \"validation_loss, validation_accuracy = model.evaluate(validation_generator, verbose=1)\\n\",\n",
    "    \"print(f\\\"\\\\nValidation Accuracy: {validation_accuracy:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Validation Loss: {validation_loss:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate predictions for confusion matrix\\n\",\n",
    "    \"validation_generator.reset()\\n\",\n",
    "    \"predictions = model.predict(validation_generator, verbose=1)\\n\",\n",
    "    \"predicted_classes = np.argmax(predictions, axis=1)\\n\",\n",
    "    \"true_classes = validation_generator.classes\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create confusion matrix\\n\",\n",
    "    \"from sklearn.metrics import confusion_matrix, classification_report\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"cm = confusion_matrix(true_classes, predicted_classes)\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "    \"            xticklabels=WASTE_CATEGORIES, \\n\",\n",
    "    \"            yticklabels=WASTE_CATEGORIES)\\n\",\n",
    "    \"plt.title('Confusion Matrix')\\n\",\n",
    "    \"plt.xlabel('Predicted')\\n\",\n",
    "    \"plt.ylabel('Actual')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification report\\n\",\n",
    "    \"print(\\\"\\\\nClassification Report:\\\")\\n\",\n",
    "    \"print(classification_report(true_classes, predicted_classes, \\n\",\n",
    "    \"                          target_names=WASTE_CATEGORIES))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Save Model and Training History\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the trained model\\n\",\n",
    "    \"model.save('waste_classification_model.h5')\\n\",\n",
    "    \"print(\\\"Model saved as 'waste_classification_model.h5'\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save training history\\n\",\n",
    "    \"with open('training_history.pkl', 'wb') as f:\\n\",\n",
    "    \"    pickle.dump(history.history, f)\\n\",\n",
    "    \"print(\\\"Training history saved as 'training_history.pkl'\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model information\\n\",\n",
    "    \"model_info = {\\n\",\n",
    "    \"    'categories': WASTE_CATEGORIES,\\n\",\n",
    "    \"    'img_size': IMG_SIZE,\\n\",\n",
    "    \"    'num_classes': len(WASTE_CATEGORIES),\\n\",\n",
    "    \"    'final_accuracy': final_val_acc,\\n\",\n",
    "    \"    'final_loss': final_val_loss\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('model_info.pkl', 'wb') as f:\\n\",\n",
    "    \"    pickle.dump(model_info, f)\\n\",\n",
    "    \"print(\\\"Model information saved as 'model_info.pkl'\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 11. Test Prediction Function\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def predict_waste_type(model, image_path):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Predict waste type from image\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Load and preprocess image\\n\",\n",
    "    \"    img = load_img(image_path, target_size=IMG_SIZE)\\n\",\n",
    "    \"    img_array = img_to_array(img)\\n\",\n",
    "    \"    img_array = np.expand_dims(img_array, axis=0)\\n\",\n",
    "    \"    img_array /= 255.0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make prediction\\n\",\n",
    "    \"    predictions = model.predict(img_array)\\n\",\n",
    "    \"    predicted_class = np.argmax(predictions[0])\\n\",\n",
    "    \"    confidence = np.max(predictions[0])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return WASTE_CATEGORIES[predicted_class], confidence, predictions[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test prediction on sample images\\n\",\n",
    "    \"print(\\\"Testing predictions on sample images:\\\")\\n\",\n",
    "    \"for category in WASTE_CATEGORIES:\\n\",\n",
    "    \"    test_image_path = f'{dataset_path}/{category}/waste_0.jpg'\\n\",\n",
    "    \"    predicted_class, confidence, all_preds = predict_waste_type(model, test_image_path)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nActual: {category}, Predicted: {predicted_class}, Confidence: {confidence:.3f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show prediction probabilities\\n\",\n",
    "    \"    for i, cat in enumerate(WASTE_CATEGORIES):\\n\",\n",
    "    \"        print(f\\\"  {cat}: {all_preds[i]:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 12. Model Performance Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(\\\"CLEANTECH WASTE CLASSIFICATION MODEL SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Model Architecture: VGG16 Transfer Learning\\\")\\n\",\n",
    "    \"print(f\\\"Number of Classes: {len(WASTE_CATEGORIES)}\\\")\\n\",\n",
    "    \"print(f\\\"Categories: {', '.join(WASTE_CATEGORIES)}\\\")\\n\",\n",
    "    \"print(f\\\"Input Size: {IMG_SIZE}\\\")\\n\",\n",
    "    \"print(f\\\"Training Samples: {train_generator.samples}\\\")\\n\",\n",
    "    \"print(f\\\"Validation Samples: {validation_generator.samples}\\\")\\n\",\n",
    "    \"print(f\\\"Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\\\")\\n\",\n",
    "    \"print(f\\\"Final Validation Loss: {final_val_loss:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Training Time: {EPOCHS} epochs\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(\\\"\\\\nFiles created:\\\")\\n\",\n",
    "    \"print(\\\"- waste_classification_model.h5\\\")\\n\",\n",
    "    \"print(\\\"- best_waste_model.h5\\\")\\n\",\n",
    "    \"print(\\\"- training_history.pkl\\\")\\n\",\n",
    "    \"print(\\\"- model_info.pkl\\\")\\n\",\n",
    "    \"print(\\\"\\\\nReady for deployment in Flask application!\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"nbconvert_version\": \"6.4.0\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
